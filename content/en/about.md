---
title: "About the EU AI Act"
description: "A practical overview of the EU’s AI regulatory framework"
date: "2025-12-25"
language: "en"
image: "/images/eu-ai-act-hero.svg"
imageAlt: "EU flag with abstract AI circuit lines"
---

# About the EU AI Act

The European Union’s Artificial Intelligence Act (EU AI Act) is an EU regulation that sets rules for placing, putting into service, and using certain AI systems in the EU. It is designed around a risk-based approach and introduces different obligations depending on the type of system and its intended use.

This page is a high-level summary for orientation. For authoritative wording and definitions, refer to the official legal text.

Official text (EUR-Lex): https://eur-lex.europa.eu/eli/reg/2024/1689/oj

## What the Act is trying to achieve

In practical terms, the EU AI Act aims to:

- **Define when AI systems are subject to specific obligations**
- **Set requirements for certain high-impact ("high-risk") use cases**
- **Introduce transparency obligations for some systems**
- **Provide a common baseline for enforcement and compliance in the EU**

## Risk-based classification (simplified)

The Act groups systems and practices into categories that carry different requirements. A simplified view is:

### Prohibited practices (often described as “unacceptable risk”)
Certain practices are restricted or prohibited under specific conditions. Examples often cited in summaries include:
- Social scoring by public authorities
- Certain uses of real-time remote biometric identification in public spaces (with exceptions and conditions)
- Manipulative or exploitative techniques in defined contexts

### High-risk systems
Some systems used in areas that can materially affect safety or fundamental rights are classified as “high-risk” and have stronger compliance obligations. Examples often cited in summaries include:
- Critical infrastructure
- Education and vocational training
- Employment and worker management
- Access to certain essential services (e.g., credit)
- Certain law enforcement and border-management contexts
- Certain biometric systems

### Transparency obligations (often described as “limited risk”)
Some systems have specific user-facing transparency requirements, such as:
- Chatbots and conversational interfaces
- Emotion recognition or biometric categorization (in defined contexts)
- Deepfakes and synthetic media disclosures

### Other systems (often described as “minimal risk”)
Many AI applications are not subject to special obligations beyond general EU law. Examples often cited include:
- Spam filters
- Recommendation systems
- AI features in games

## Compliance and enforcement (high level)

For high-risk systems, typical compliance themes include:
- Risk management and quality management processes
- Data governance and documentation
- Transparency and information for users
- Human oversight measures
- Robustness, security, and monitoring

Penalties vary by infringement type and may include substantial administrative fines.

## Timeline (high level)

Implementation is phased. Specific dates and obligations depend on the category of system and provisions that apply.

## Why this matters (neutral framing)

If you build, deploy, procure, or integrate AI systems that may fall under the EU AI Act, a basic understanding helps you:
- scope whether a use case may be regulated
- plan compliance work early (documentation, controls, testing)
- avoid surprises during procurement, audits, or go-live
